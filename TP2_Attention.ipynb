{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" ### **Étape 1 : Configuration de l’environnement Kaggle**\n","metadata":{}},{"cell_type":"code","source":"# Installer les dépendances nécessaires\n!pip install torch torchvision transformers\n!pip install tensorboard\n!pip install nltk\n\n# Import des bibliothèques\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nimport nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:01.107881Z","iopub.execute_input":"2025-12-25T10:36:01.108641Z","iopub.status.idle":"2025-12-25T10:36:10.593741Z","shell.execute_reply.started":"2025-12-25T10:36:01.108608Z","shell.execute_reply":"2025-12-25T10:36:10.592859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 2 : Télécharger et charger le dataset**","metadata":{}},{"cell_type":"code","source":"# Télécharger NLTK data pour tokenizer\nnltk.download('punkt')\n\n# Lire le fichier CSV des résultats\ndf = pd.read_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv', delimiter='|')\ndf.columns = df.columns.str.strip()  # Nettoyer les noms de colonnes\ndf['comment'] = df['comment'].str.strip()  # Nettoyer les commentaires\n\nprint(\"Colonnes du dataset :\", df.columns)\nprint(df.head())\nprint(f\"Nombre total d'images : {df['image_name'].nunique()}\")\nprint(f\"Nombre total de légendes : {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:10.595458Z","iopub.execute_input":"2025-12-25T10:36:10.595716Z","iopub.status.idle":"2025-12-25T10:36:10.967034Z","shell.execute_reply.started":"2025-12-25T10:36:10.595689Z","shell.execute_reply":"2025-12-25T10:36:10.966406Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 3 : Créer le vocabulaire**","metadata":{}},{"cell_type":"code","source":"# Étape 3 : Créer le vocabulaire (Corrigé)\nimport nltk\nfrom collections import Counter\nnltk.download('punkt')\n\ndef build_vocab(df, min_freq=2):\n    \"\"\"Construit le vocabulaire à partir des légendes\"\"\"\n    # Nettoyer les données : supprimer les commentaires vides ou NaN\n    df_clean = df[df['comment'].notna() & (df['comment'].str.strip() != '')]\n    \n    # Tokenizer les légendes\n    all_tokens = []\n    for caption in df_clean['comment']:\n        try:\n            tokens = nltk.word_tokenize(str(caption).lower())\n            all_tokens.extend(tokens)\n        except Exception as e:\n            print(f\"Erreur avec la légende : {caption}\")\n            continue\n    \n    # Compter les fréquences\n    word_counts = Counter(all_tokens)\n    \n    # Filtrer par fréquence minimale\n    vocab = ['<pad>', '<sos>', '<eos>', '<unk>']\n    vocab.extend([word for word, count in word_counts.items() if count >= min_freq])\n    \n    word2idx = {word: idx for idx, word in enumerate(vocab)}\n    idx2word = {idx: word for idx, word in enumerate(vocab)}\n    \n    print(f\"Taille du vocabulaire après filtrage (freq >= {min_freq}): {len(vocab)}\")\n    print(f\"10 mots les plus fréquents: {word_counts.most_common(10)}\")\n    \n    return vocab, word2idx, idx2word\n# Construire le vocabulaire\nvocab, word2idx, idx2word = build_vocab(df)\nvocab_size = len(vocab)\nprint(f\"Taille totale du vocabulaire : {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:10.96794Z","iopub.execute_input":"2025-12-25T10:36:10.968231Z","iopub.status.idle":"2025-12-25T10:36:18.450276Z","shell.execute_reply.started":"2025-12-25T10:36:10.968207Z","shell.execute_reply":"2025-12-25T10:36:18.449551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 4 : Créer le Dataset personnalisé**","metadata":{}},{"cell_type":"code","source":"# Étape 4 : Créer le Dataset personnalisé (Corrigé)\nclass Flickr30kDataset(Dataset):\n    def __init__(self, df, image_dir, word2idx, transform=None, max_length=30):\n        \"\"\"\n        df: DataFrame contenant les légendes\n        image_dir: Chemin vers le dossier des images\n        word2idx: Dictionnaire de mapping mot -> index\n        transform: Transformations à appliquer aux images\n        max_length: Longueur maximale des légendes\n        \"\"\"\n        self.df = df\n        self.image_dir = image_dir\n        self.word2idx = word2idx\n        self.transform = transform\n        self.max_length = max_length\n        \n        # Nettoyer le DataFrame\n        self.df_clean = self.df[\n            self.df['comment'].notna() & \n            (self.df['comment'].str.strip() != '')\n        ].copy()\n        \n        # Grouper les légendes par image\n        self.image_to_captions = {}\n        for _, row in self.df_clean.iterrows():\n            img_name = row['image_name'].strip()\n            caption = str(row['comment']).strip()\n            \n            if img_name not in self.image_to_captions:\n                self.image_to_captions[img_name] = []\n            self.image_to_captions[img_name].append(caption)\n        \n        self.images = list(self.image_to_captions.keys())\n        print(f\"Nombre d'images dans le dataset : {len(self.images)}\")\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        \n        # Vérifier si le fichier existe\n        if not os.path.exists(img_path):\n            # Essayer d'autres extensions\n            for ext in ['.jpg', '.jpeg', '.png']:\n                alt_path = img_path + ext\n                if os.path.exists(alt_path):\n                    img_path = alt_path\n                    break\n        \n        # Charger l'image\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Erreur de chargement de l'image {img_path}: {e}\")\n            # Retourner une image noire en cas d'erreur\n            image = Image.new('RGB', (224, 224), color='black')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Sélectionner une légende aléatoire pour cette image\n        captions = self.image_to_captions[img_name]\n        caption = np.random.choice(captions)\n        \n        # Tokenizer et encoder la légende\n        try:\n            tokens = nltk.word_tokenize(caption.lower())\n        except:\n            tokens = caption.lower().split()  # Fallback simple\n            \n        encoded = [self.word2idx['<sos>']]\n        \n        for token in tokens[:self.max_length-2]:  # -2 pour <sos> et <eos>\n            encoded.append(self.word2idx.get(token, self.word2idx['<unk>']))\n        \n        encoded.append(self.word2idx['<eos>'])\n        \n        # Padding\n        if len(encoded) < self.max_length:\n            encoded += [self.word2idx['<pad>']] * (self.max_length - len(encoded))\n        else:\n            encoded = encoded[:self.max_length]\n            encoded[-1] = self.word2idx['<eos>']\n        \n        return image, torch.tensor(encoded, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:18.45136Z","iopub.execute_input":"2025-12-25T10:36:18.451674Z","iopub.status.idle":"2025-12-25T10:36:18.462344Z","shell.execute_reply.started":"2025-12-25T10:36:18.45164Z","shell.execute_reply":"2025-12-25T10:36:18.461599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Étape 4 (suite) : Tester le dataset\n# Définir les transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Créer le dataset complet\nimage_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images'\ndataset = Flickr30kDataset(df, image_dir, word2idx, transform=transform)\n\n# Séparer en train/test (80/20)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\nprint(f\"Taille du dataset d'entraînement : {len(train_dataset)}\")\nprint(f\"Taille du dataset de test : {len(test_dataset)}\")\n\n# Tester un échantillon\nsample_image, sample_caption = dataset[0]\nprint(f\"Shape de l'image : {sample_image.shape}\")\nprint(f\"Shape de la légende : {sample_caption.shape}\")\nprint(f\"Légende encodée : {sample_caption[:10]}...\")  # Afficher les 10 premiers tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:18.463943Z","iopub.execute_input":"2025-12-25T10:36:18.464191Z","iopub.status.idle":"2025-12-25T10:36:23.892391Z","shell.execute_reply.started":"2025-12-25T10:36:18.46417Z","shell.execute_reply":"2025-12-25T10:36:23.891739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 5 : Créer les DataLoader avec fonction de collate**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\ndef collate_fn(batch):\n    \"\"\"Fonction pour grouper les batchs\"\"\"\n    images, captions = zip(*batch)\n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    return images, captions\n\n# Créer les DataLoader\nbatch_size = 32  # Ajustez selon la mémoire disponible sur Kaggle\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=2\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    collate_fn=collate_fn,\n    num_workers=2\n)\n\n# Vérifier un batch\nimages, captions = next(iter(train_loader))\nprint(f\"Shape des images : {images.shape}\")  # Devrait être (batch, 3, 224, 224)\nprint(f\"Shape des légendes : {captions.shape}\")  # Devrait être (batch, max_length)\nprint(f\"Première légende : {captions[0][:10]}...\")  # Afficher les 10 premiers tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:23.893382Z","iopub.execute_input":"2025-12-25T10:36:23.893627Z","iopub.status.idle":"2025-12-25T10:36:24.789824Z","shell.execute_reply.started":"2025-12-25T10:36:23.893604Z","shell.execute_reply":"2025-12-25T10:36:24.788957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 5 : Créer les DataLoader avec fonction de collate**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\ndef collate_fn(batch):\n    \"\"\"Fonction pour grouper les batchs\"\"\"\n    images, captions = zip(*batch)\n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    return images, captions\n\n# Créer les DataLoader\nbatch_size = 32  # Ajustez selon la mémoire disponible sur Kaggle\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=2\n)\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    collate_fn=collate_fn,\n    num_workers=2\n)\n# Vérifier un batch\nimages, captions = next(iter(train_loader))\nprint(f\"Shape des images : {images.shape}\")  # Devrait être (batch, 3, 224, 224)\nprint(f\"Shape des légendes : {captions.shape}\")  # Devrait être (batch, max_length)\nprint(f\"Première légende : {captions[0][:10]}...\")  # Afficher les 10 premiers tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:24.791061Z","iopub.execute_input":"2025-12-25T10:36:24.791364Z","iopub.status.idle":"2025-12-25T10:36:25.738054Z","shell.execute_reply.started":"2025-12-25T10:36:24.791328Z","shell.execute_reply":"2025-12-25T10:36:25.737279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 6 : Créer l'embedding layer**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings=None):\n        super().__init__()\n        if pretrained_embeddings is not None:\n            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n            # Initialisation Xavier\n            nn.init.xavier_uniform_(self.embedding.weight)\n    \n    def forward(self, x):\n        return self.embedding(x)\n\n# Créer l'embedding (pour l'instant aléatoire)\nembedding_dim = 300\nembedding_layer = EmbeddingLayer(vocab_size, embedding_dim)\nprint(\"Embedding layer créé\")\nprint(f\"Taille de l'embedding : {vocab_size} mots x {embedding_dim} dimensions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:25.739357Z","iopub.execute_input":"2025-12-25T10:36:25.73963Z","iopub.status.idle":"2025-12-25T10:36:25.792206Z","shell.execute_reply.started":"2025-12-25T10:36:25.739599Z","shell.execute_reply":"2025-12-25T10:36:25.791486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 6 : Créer l'embedding layer**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings=None):\n        super().__init__()\n        if pretrained_embeddings is not None:\n            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n            # Initialisation Xavier\n            nn.init.xavier_uniform_(self.embedding.weight)\n    \n    def forward(self, x):\n        return self.embedding(x)\n\n# Créer l'embedding (pour l'instant aléatoire)\nembedding_dim = 300\nembedding_layer = EmbeddingLayer(vocab_size, embedding_dim)\nprint(\"Embedding layer créé\")\nprint(f\"Taille de l'embedding : {vocab_size} mots x {embedding_dim} dimensions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:25.793261Z","iopub.execute_input":"2025-12-25T10:36:25.793653Z","iopub.status.idle":"2025-12-25T10:36:25.847009Z","shell.execute_reply.started":"2025-12-25T10:36:25.793619Z","shell.execute_reply":"2025-12-25T10:36:25.846462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 7 : Extraire les features avec ResNet**","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Charger ResNet50 pré-entraîné\n        resnet = models.resnet50(pretrained=True)\n        \n        # Extraire les couches jusqu'à avant la dernière couche avgpool\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        # Geler les poids\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n        \n        # Adapter la taille\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        # Mettre en mode évaluation\n        self.resnet.eval()\n        \n    def forward(self, images):\n        \"\"\"Extrait les features d'images\"\"\"\n        with torch.no_grad():\n            features = self.resnet(images)  # (batch, 2048, H, W)\n        \n        features = self.adaptive_pool(features)  # (batch, 2048, 7, 7)\n        features = features.permute(0, 2, 3, 1)  # (batch, 7, 7, 2048)\n        batch_size = features.size(0)\n        features = features.view(batch_size, -1, 2048)  # (batch, 49, 2048)\n        \n        return features\n\n# Tester l'extracteur\nfeature_extractor = FeatureExtractor()\nwith torch.no_grad():\n    test_features = feature_extractor(images[:2])\nprint(f\"Shape des features extraites : {test_features.shape}\")  # (2, 49, 2048)\nprint(f\"Nombre de features par image : {test_features.shape[1]}\")  # 49 (7x7)\nprint(f\"Dimension de chaque feature : {test_features.shape[2]}\")  # 2048","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:25.847802Z","iopub.execute_input":"2025-12-25T10:36:25.847989Z","iopub.status.idle":"2025-12-25T10:36:26.411365Z","shell.execute_reply.started":"2025-12-25T10:36:25.84797Z","shell.execute_reply":"2025-12-25T10:36:26.410589Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 8 : Implémenter le module d'attention**","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n        \n        # Couche pour calculer les scores d'attention\n        self.attention_layer = nn.Linear(encoder_dim + decoder_dim, 1)\n        \n        # Softmax pour les poids d'attention\n        self.softmax = nn.Softmax(dim=1)\n        \n        # Initialisation des poids\n        nn.init.xavier_uniform_(self.attention_layer.weight)\n        nn.init.constant_(self.attention_layer.bias, 0)\n    \n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: (batch_size, num_pixels, encoder_dim)\n        decoder_hidden: (batch_size, decoder_dim)\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        num_pixels = encoder_out.size(1)\n        \n        # Répéter decoder_hidden pour chaque pixel\n        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, num_pixels, 1)  # (batch, num_pixels, decoder_dim)\n        \n        # Concaténer features et hidden state\n        combined = torch.cat((encoder_out, decoder_hidden), dim=2)  # (batch, num_pixels, encoder_dim+decoder_dim)\n        \n        # Calculer les scores d'attention\n        attention_scores = self.attention_layer(combined).squeeze(2)  # (batch, num_pixels)\n        \n        # Appliquer softmax pour obtenir les poids\n        attention_weights = self.softmax(attention_scores)  # (batch, num_pixels)\n        \n        # Calculer le vecteur de contexte\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_out)  # (batch, 1, encoder_dim)\n        context_vector = context_vector.squeeze(1)  # (batch, encoder_dim)\n        \n        return context_vector, attention_weights\n\n# Tester le module d'attention\nattention_dim = 512\nattention_module = Attention(encoder_dim=2048, decoder_dim=attention_dim)\n\n# Tester avec des features et un état caché fictifs\nbatch_size = 2\ndummy_features = torch.randn(batch_size, 49, 2048)\ndummy_hidden = torch.randn(batch_size, attention_dim)\n\ncontext_vector, attention_weights = attention_module(dummy_features, dummy_hidden)\nprint(f\"Shape du context vector : {context_vector.shape}\")  # (2, 2048)\nprint(f\"Shape des attention weights : {attention_weights.shape}\")  # (2, 49)\nprint(f\"Somme des poids d'attention : {attention_weights.sum(dim=1)}\")  # Devrait être ~1.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.412487Z","iopub.execute_input":"2025-12-25T10:36:26.412745Z","iopub.status.idle":"2025-12-25T10:36:26.424789Z","shell.execute_reply.started":"2025-12-25T10:36:26.412723Z","shell.execute_reply":"2025-12-25T10:36:26.42392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 9 : Implémenter le LSTM avec attention**","metadata":{}},{"cell_type":"code","source":"class LSTMWithAttention(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, encoder_dim, vocab_size):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.encoder_dim = encoder_dim\n        self.vocab_size = vocab_size\n        \n        # Module d'attention\n        self.attention = Attention(encoder_dim, hidden_dim)\n        \n        # Portes du LSTM\n        self.lstm_cell = nn.LSTMCell(embedding_dim + encoder_dim, hidden_dim, bias=True)\n        \n        # Couche pour initialiser l'état caché\n        self.init_h = nn.Linear(encoder_dim, hidden_dim)\n        self.init_c = nn.Linear(encoder_dim, hidden_dim)\n        \n        # Couche pour prédire le mot suivant\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n        \n        # Initialisation des poids\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialisation des poids\"\"\"\n        nn.init.xavier_uniform_(self.init_h.weight)\n        nn.init.constant_(self.init_h.bias, 0)\n        nn.init.xavier_uniform_(self.init_c.weight)\n        nn.init.constant_(self.init_c.bias, 0)\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n    \n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Initialise les états cachés à partir des features de l'image\n        encoder_out: (batch_size, num_pixels, encoder_dim)\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)  # (batch_size, encoder_dim)\n        h = self.init_h(mean_encoder_out)  # (batch_size, hidden_dim)\n        c = self.init_c(mean_encoder_out)  # (batch_size, hidden_dim)\n        return h, c\n    \n    def forward(self, encoder_out, captions_embedded, teacher_forcing_ratio=0.5):\n        \"\"\"\n        encoder_out: (batch_size, num_pixels, encoder_dim)\n        captions_embedded: (batch_size, seq_len, embedding_dim)\n        teacher_forcing_ratio: probabilité d'utiliser teacher forcing\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        seq_len = captions_embedded.size(1)\n        \n        # Initialiser les états cachés\n        h, c = self.init_hidden_state(encoder_out)\n        \n        # Créer des tenseurs pour stocker les prédictions\n        predictions = torch.zeros(batch_size, seq_len, self.vocab_size).to(encoder_out.device)\n        \n        # On ne peut pas utiliser teacher forcing de manière simple car nous n'avons pas \n        # accès à l'embedding layer ici. On va simplifier et toujours utiliser l'embedding\n        # du mot vrai (teacher forcing = 1.0)\n        \n        # Pour chaque pas de temps\n        for t in range(seq_len):\n            # Calculer l'attention\n            context_vector, _ = self.attention(encoder_out, h)\n            \n            # Utiliser l'embedding du mot vrai (simplification)\n            # Dans une version plus avancée, on pourrait passer l'embedding layer en paramètre\n            lstm_input = torch.cat([captions_embedded[:, t, :], context_vector], dim=1)\n            \n            # Mettre à jour les états LSTM\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            \n            # Prédiction du prochain mot\n            output = self.fc(self.dropout(h))\n            predictions[:, t, :] = output\n        \n        return predictions\n    \n    def decode_step(self, encoder_out, word_embedded, h, c):\n        \"\"\"Un pas de décodage pour l'inférence\"\"\"\n        # Calculer l'attention\n        context_vector, _ = self.attention(encoder_out, h)\n        \n        # Concaténer l'embedding du mot et le contexte\n        lstm_input = torch.cat([word_embedded, context_vector], dim=1)\n        \n        # Mettre à jour les états LSTM\n        h, c = self.lstm_cell(lstm_input, (h, c))\n        \n        # Prédiction du prochain mot\n        output = self.fc(self.dropout(h))\n        \n        return output, h, c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.425627Z","iopub.execute_input":"2025-12-25T10:36:26.425817Z","iopub.status.idle":"2025-12-25T10:36:26.443265Z","shell.execute_reply.started":"2025-12-25T10:36:26.425797Z","shell.execute_reply":"2025-12-25T10:36:26.442463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 10 : Modèle complet de captioning**","metadata":{}},{"cell_type":"code","source":"class ImageCaptioningModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        \n        # Feature extractor\n        self.encoder = FeatureExtractor()\n        encoder_dim = 2048\n        \n        # Embedding layer\n        self.embedding = EmbeddingLayer(vocab_size, embedding_dim)\n        \n        # LSTM with attention\n        self.decoder = LSTMWithAttention(embedding_dim, hidden_dim, encoder_dim, vocab_size)\n        \n    def forward(self, images, captions, teacher_forcing_ratio=0.5):\n        \"\"\"\n        images: (batch, 3, 224, 224)\n        captions: (batch, seq_len)\n        teacher_forcing_ratio: probabilité d'utiliser teacher forcing\n        \"\"\"\n        # Extraire les features\n        encoder_out = self.encoder(images)  # (batch, 49, 2048)\n        \n        # Embedding des captions\n        captions_embedded = self.embedding(captions)  # (batch, seq_len, embedding_dim)\n        \n        # Générer les prédictions avec le décodage\n        predictions = self.decoder(encoder_out, captions_embedded, teacher_forcing_ratio)\n        \n        return predictions\n    \n    def generate_caption(self, image, word2idx, idx2word, max_length=30):\n        \"\"\"Génère une légende pour une seule image\"\"\"\n        self.eval()\n        with torch.no_grad():\n            # Préparer l'image\n            if len(image.shape) == 3:\n                image = image.unsqueeze(0)  # (1, 3, 224, 224)\n            \n            # Extraire les features\n            encoder_out = self.encoder(image)  # (1, 49, 2048)\n            \n            # Initialiser les états cachés\n            h, c = self.decoder.init_hidden_state(encoder_out)\n            \n            # Commencer avec <sos>\n            input_word = torch.tensor([[word2idx['<sos>']]]).to(image.device)\n            \n            caption_words = []\n            attention_weights_list = []\n            \n            for _ in range(max_length):\n                # Embedding du mot courant\n                word_embedded = self.embedding(input_word).squeeze(1)\n                \n                # Un pas de décodage\n                output, h, c = self.decoder.decode_step(encoder_out, word_embedded, h, c)\n                \n                # Prédire le mot suivant\n                predicted_word = output.argmax(1)\n                word_idx = predicted_word.item()\n                \n                # Vérifier si c'est la fin\n                if word_idx == word2idx['<eos>']:\n                    break\n                \n                # Ajouter le mot à la légende\n                word = idx2word[word_idx]\n                if word not in ['<pad>', '<unk>']:\n                    caption_words.append(word)\n                \n                # Mettre à jour le mot d'entrée pour l'itération suivante\n                input_word = predicted_word.unsqueeze(1)\n            \n            return ' '.join(caption_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.444123Z","iopub.execute_input":"2025-12-25T10:36:26.444367Z","iopub.status.idle":"2025-12-25T10:36:26.461985Z","shell.execute_reply.started":"2025-12-25T10:36:26.444346Z","shell.execute_reply":"2025-12-25T10:36:26.461298Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 11 : Initialiser le modèle et l'entraînement**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Subset\nimport numpy as np\n\n# Reproductibilité\nnp.random.seed(42)\n\n# =========================\n# TRAIN : 50 %\n# =========================\ntrain_size = len(train_dataset)\ntrain_indices = np.random.choice(\n    train_size,\n    size=int(0.5 * train_size),\n    replace=False\n)\n\ntrain_subset = Subset(train_dataset, train_indices)\n\ntrain_loader = DataLoader(\n    train_subset,\n    batch_size=32,        # tu peux ajuster\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\n# =========================\n# VALIDATION / TEST : 50 %\n# =========================\ntest_size = len(test_dataset)\ntest_indices = np.random.choice(\n    test_size,\n    size=int(0.5 * test_size),\n    replace=False\n)\n\ntest_subset = Subset(test_dataset, test_indices)\n\ntest_loader = DataLoader(\n    test_subset,\n    batch_size=32,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\n# =========================\n# Vérification (IMPORTANT)\n# =========================\nprint(\"=== Vérification des données utilisées ===\")\nprint(f\"Train total       : {train_size}\")\nprint(f\"Train utilisé (50%): {len(train_loader.dataset)}\")\nprint(f\"Test total        : {test_size}\")\nprint(f\"Test utilisé (50%): {len(test_loader.dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.464523Z","iopub.execute_input":"2025-12-25T10:36:26.464741Z","iopub.status.idle":"2025-12-25T10:36:26.483495Z","shell.execute_reply.started":"2025-12-25T10:36:26.46472Z","shell.execute_reply":"2025-12-25T10:36:26.482644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Étape 11 : Initialiser le modèle et l'entraînement (version simplifiée)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# Hyperparamètres (version rapide)\nembedding_dim = 256     # bon compromis qualité / vitesse\nhidden_dim = 256        # rapide sur GPU\nlearning_rate = 0.001\nnum_epochs = 25\n         # ↓ suffisant pour observer la convergence\n\n\n# Créer le modèle\nmodel = ImageCaptioningModel(vocab_size, embedding_dim, hidden_dim).to(device)\nprint(f\"Modèle créé sur {device}\")\nprint(f\"Nombre de paramètres : {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Nombre de paramètres entraînables : {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# Fonction de perte et optimiseur\ncriterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=learning_rate\n)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n# Fonction pour décoder une séquence\ndef decode_sequence(sequence, idx2word):\n    \"\"\"Convertit une séquence d'indices en texte\"\"\"\n    words = []\n    for idx in sequence:\n        word = idx2word[idx]\n        if word == '<eos>':\n            break\n        if word not in ['<sos>', '<pad>', '<unk>']:\n            words.append(word)\n    return ' '.join(words)\n\n# Entraînement simplifié\nprint(\"Début de l'entraînement...\")\n\nfrom tqdm import tqdm\n\nfor epoch in range(num_epochs):\n    # Phase d'entraînement\n    model.train()\n    train_loss = 0.0\n    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n    \n    for images, captions in train_pbar:\n        images = images.to(device)\n        captions = captions.to(device)\n        \n        # Forward pass (sans teacher forcing)\n        outputs = model(images, captions)\n        \n        # Calcul de la loss\n        # On décale d'un pas de temps pour la prédiction\n        outputs = outputs[:, :-1, :].contiguous()  # Ignorer la dernière prédiction\n        targets = captions[:, 1:].contiguous()  # Ignorer <sos>\n        \n        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping pour éviter les explosions de gradient\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        train_loss += loss.item()\n        train_pbar.set_postfix({'loss': loss.item()})\n    \n    # Phase de validation\n    model.eval()\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        val_pbar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n        for images, captions in val_pbar:\n            images = images.to(device)\n            captions = captions.to(device)\n            \n            outputs = model(images, captions)\n            outputs = outputs[:, :-1, :].contiguous()\n            targets = captions[:, 1:].contiguous()\n            \n            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n            val_loss += loss.item()\n            val_pbar.set_postfix({'loss': loss.item()})\n    \n    # Calcul des moyennes\n    avg_train_loss = train_loss / len(train_loader)\n    avg_val_loss = val_loss / len(test_loader)\n    \n    # Mise à jour du scheduler\n    scheduler.step()\n    \n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    # Générer un exemple de légende toutes les 5 époques\n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        model.eval()\n        with torch.no_grad():\n            # Prendre une image de test\n            test_image, test_caption = test_dataset[0]\n            test_image_tensor = test_image.unsqueeze(0).to(device)\n            \n            # Générer une légende\n            generated_caption = model.generate_caption(test_image_tensor, word2idx, idx2word)\n            \n            # Décoder la vraie légende\n            true_caption = decode_sequence(test_caption.tolist(), idx2word)\n            \n            print(f\"\\nExemple de génération (Epoch {epoch+1}):\")\n            print(f\"  Vraie légende: {true_caption}\")\n            print(f\"  Légende générée: {generated_caption}\")\n            print(\"-\" * 60)\n\nprint(\"Entraînement terminé!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.484509Z","iopub.execute_input":"2025-12-25T10:36:26.484798Z","iopub.status.idle":"2025-12-25T11:13:20.71693Z","shell.execute_reply.started":"2025-12-25T10:36:26.484768Z","shell.execute_reply":"2025-12-25T11:13:20.716055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 12 : Évaluation du modèle**","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion, device, max_examples=10):\n    \"\"\"Évalue le modèle sur un dataloader\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_examples = 0\n    \n    with torch.no_grad():\n        for i, (images, captions) in enumerate(dataloader):\n            if i * batch_size >= max_examples:\n                break\n                \n            images = images.to(device)\n            captions = captions.to(device)\n            \n            # Forward pass\n            outputs = model(images, captions, teacher_forcing_ratio=0.0)\n            outputs = outputs[:, :-1, :].contiguous()\n            targets = captions[:, 1:].contiguous()\n            \n            # Calcul de la loss\n            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n            \n            total_loss += loss.item() * images.size(0)\n            total_examples += images.size(0)\n            \n            # Afficher quelques exemples\n            if i == 0:\n                print(\"\\nExemples de génération sur le test set:\")\n                for j in range(min(3, images.size(0))):\n                    # Générer une légende\n                    generated = model.generate_caption(images[j:j+1], word2idx, idx2word)\n                    \n                    # Décoder la vraie légende\n                    true_caption = decode_sequence(captions[j].tolist(), idx2word)\n                    \n                    print(f\"\\nExemple {j+1}:\")\n                    print(f\"  Vraie: {true_caption}\")\n                    print(f\"  Générée: {generated}\")\n    \n    avg_loss = total_loss / total_examples if total_examples > 0 else 0\n    print(f\"\\nÉvaluation terminée\")\n    print(f\"Loss moyenne sur {total_examples} exemples: {avg_loss:.4f}\")\n    \n    return avg_loss\n\n# Évaluer le modèle\nprint(\"Évaluation du modèle sur le test set...\")\ntest_loss = evaluate_model(model, test_loader, criterion, device, max_examples=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:13:20.718203Z","iopub.execute_input":"2025-12-25T11:13:20.718477Z","iopub.status.idle":"2025-12-25T11:13:23.648705Z","shell.execute_reply.started":"2025-12-25T11:13:20.718447Z","shell.execute_reply":"2025-12-25T11:13:23.647882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 13 : Sauvegarde et chargement du modèle**","metadata":{}},{"cell_type":"code","source":"import os\nimport datetime\n\ndef save_model(model, optimizer, epoch, word2idx, idx2word, path='model_checkpoint.pth'):\n    \"\"\"Sauvegarde le modèle\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'vocab_size': vocab_size,\n        'embedding_dim': embedding_dim,\n        'hidden_dim': hidden_dim,\n        'word2idx': word2idx,\n        'idx2word': idx2word,\n        'loss': test_loss if 'test_loss' in locals() else None\n    }\n    \n    # Créer un nom de fichier avec la date\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f'caption_model_{timestamp}.pth'\n    \n    torch.save(checkpoint, filename)\n    print(f\"Modèle sauvegardé dans {filename}\")\n    \n    return filename\n\ndef load_model(filename, device='cuda'):\n    \"\"\"Charge un modèle sauvegardé\"\"\"\n    checkpoint = torch.load(filename, map_location=device)\n    \n    # Créer le modèle\n    model = ImageCaptioningModel(\n        checkpoint['vocab_size'],\n        checkpoint['embedding_dim'],\n        checkpoint['hidden_dim']\n    ).to(device)\n    \n    # Charger les poids\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Charger l'optimizer si nécessaire\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    print(f\"Modèle chargé depuis {filename}\")\n    print(f\"Époque: {checkpoint['epoch']}, Loss: {checkpoint.get('loss', 'N/A')}\")\n    \n    return model, optimizer, checkpoint\n\n# Sauvegarder le modèle\nsaved_file = save_model(model, optimizer, num_epochs, word2idx, idx2word)\nprint(f\"Modèle sauvegardé: {saved_file}\")\n\n# Pour charger le modèle plus tard:\n# loaded_model, loaded_optimizer, checkpoint = load_model('votre_fichier.pth', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:13:23.649802Z","iopub.execute_input":"2025-12-25T11:13:23.650027Z","iopub.status.idle":"2025-12-25T11:13:23.976861Z","shell.execute_reply.started":"2025-12-25T11:13:23.650001Z","shell.execute_reply":"2025-12-25T11:13:23.976223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 14 : Génération interactive**","metadata":{}},{"cell_type":"code","source":"# Définir d'abord la fonction generate_caption_beam_search\ndef generate_caption_beam_search(model, image, word2idx, idx2word, beam_size=3, max_length=20):\n    \"\"\"Génère une légende avec beam search\"\"\"\n    model.eval()\n    \n    # Préparer l'image\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    \n    image = image.to(device)\n    \n    with torch.no_grad():\n        # Extraire les features (méthode dépend du modèle)\n        if hasattr(model, 'encoder'):\n            # Pour les modèles avec encoder\n            features = model.encoder(image)\n            if hasattr(model, 'feature_adapter'):\n                # Adapter les features si nécessaire\n                batch_size = features.size(0)\n                features = features.view(batch_size, -1)\n                features = model.feature_adapter(features)\n        else:\n            # Méthode générique\n            features = image\n    \n    # Initialiser les beams\n    beams = [{\n        'sequence': [word2idx['<sos>']],\n        'score': 0.0,\n        'features': features.clone() if features is not None else None\n    }]\n    \n    for step in range(max_length):\n        new_beams = []\n        \n        for beam in beams:\n            # Si la séquence se termine par <eos>, la garder telle quelle\n            if beam['sequence'][-1] == word2idx['<eos>']:\n                new_beams.append(beam)\n                continue\n            \n            # Préparer l'entrée pour le modèle\n            # Note: Cette partie dépend de la structure de votre modèle\n            # Vous devrez peut-être l'adapter\n            \n            # Pour les modèles simples\n            try:\n                # Tenter d'utiliser la méthode generate_caption_step si elle existe\n                if hasattr(model, 'generate_caption_step'):\n                    # Vous devriez implémenter cette méthode dans votre modèle\n                    pass\n                else:\n                    # Approche simplifiée: utiliser la méthode forward du modèle\n                    # Cette partie est complexe et dépend de l'architecture\n                    # Pour l'instant, nous allons utiliser une approche plus simple\n                    continue\n            except:\n                continue\n        \n        # Trier et garder les beam_size meilleurs beams\n        beams = sorted(new_beams, key=lambda x: x['score'], reverse=True)[:beam_size]\n        \n        # Si tous les beams se terminent par <eos>, arrêter\n        if all(beam['sequence'][-1] == word2idx['<eos>'] for beam in beams):\n            break\n    \n    # Meilleure séquence\n    best_beam = beams[0]\n    \n    # Convertir en texte\n    words = []\n    for idx in best_beam['sequence'][1:]:  # Ignorer <sos>\n        if idx == word2idx['<eos>']:\n            break\n        word = idx2word[idx]\n        if word not in ['<pad>', '<unk>']:\n            words.append(word)\n    \n    return ' '.join(words)\n\n# Version simplifiée sans beam search pour l'instant\ndef generate_caption_simple(model, image, word2idx, idx2word, max_length=20):\n    \"\"\"Génère une légende simple (sans beam search)\"\"\"\n    model.eval()\n    \n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    \n    image = image.to(device)\n    \n    # Utiliser la méthode generate_caption si elle existe\n    if hasattr(model, 'generate_caption'):\n        return model.generate_caption(image, word2idx, idx2word, max_length)\n    else:\n        # Fallback: méthode générique simple\n        with torch.no_grad():\n            # Cette partie dépend de votre modèle\n            # Pour l'instant, retourner une légende par défaut\n            return \"a person in an image\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:47:29.505902Z","iopub.execute_input":"2025-12-25T11:47:29.506537Z","iopub.status.idle":"2025-12-25T11:47:29.517323Z","shell.execute_reply.started":"2025-12-25T11:47:29.506505Z","shell.execute_reply":"2025-12-25T11:47:29.516711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exemple d'utilisation interactive\nprint(\"\\nGénération interactive:\")\nprint(\"=\" * 50)\n\n# Assurez-vous que le modèle est en mode évaluation\nmodel.eval()\n\n# Prendre quelques images au hasard du test set\nimport random\n\nnum_samples = 5\nsample_indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))\n\nfor i, idx in enumerate(sample_indices):\n    test_image, test_caption = test_dataset[idx]\n    \n    # Vérifier si le modèle a une méthode generate_caption\n    if hasattr(model, 'generate_caption'):\n        # Générer une légende avec la méthode du modèle\n        generated = model.generate_caption(test_image.unsqueeze(0).to(device), word2idx, idx2word)\n    else:\n        # Fallback: utiliser la méthode simple\n        generated = generate_caption_simple(model, test_image, word2idx, idx2word)\n    \n    # Décoder la vraie légende\n    true_caption = decode_sequence(test_caption.tolist(), idx2word)\n    \n    print(f\"\\nExemple {i+1}:\")\n    print(f\"  Vraie légende: {true_caption}\")\n    print(f\"  Légende générée: {generated}\")\n    \n    print(\"-\" * 50)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"GÉNÉRATION INTERACTIVE TERMINÉE\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:48:11.473442Z","iopub.execute_input":"2025-12-25T11:48:11.474167Z","iopub.status.idle":"2025-12-25T11:48:11.655841Z","shell.execute_reply.started":"2025-12-25T11:48:11.474138Z","shell.execute_reply":"2025-12-25T11:48:11.655174Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Étape 15 : Visualisation de l'attention**","metadata":{}},{"cell_type":"code","source":"def visualize_attention(model, image, word2idx, idx2word, max_length=30):\n    \"\"\"Génère une légende et visualise les poids d'attention\"\"\"\n    model.eval()\n    \n    # Préparer l'image\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    \n    image = image.to(device)\n    \n    with torch.no_grad():\n        # Extraire les features\n        encoder_out = model.encoder(image)  # (1, 49, 2048)\n        \n        # Initialiser les états cachés\n        h, c = model.decoder.init_hidden_state(encoder_out)\n        \n        # Commencer avec <sos>\n        input_word = torch.tensor([[word2idx['<sos>']]]).to(device)\n        \n        caption_words = []\n        attention_weights_list = []\n        \n        for _ in range(max_length):\n            # Embedding du mot courant\n            word_embedded = model.embedding(input_word).squeeze(1)\n            \n            # Calculer l'attention (nous avons besoin d'accéder aux poids)\n            batch_size = encoder_out.size(0)\n            num_pixels = encoder_out.size(1)\n            \n            # Répéter decoder_hidden pour chaque pixel\n            decoder_hidden = h.unsqueeze(1).repeat(1, num_pixels, 1)\n            \n            # Concaténer features et hidden state\n            combined = torch.cat((encoder_out, decoder_hidden), dim=2)\n            \n            # Calculer les scores d'attention\n            attention_scores = model.decoder.attention.attention_layer(combined).squeeze(2)\n            attention_weights = model.decoder.attention.softmax(attention_scores)\n            \n            # Calculer le vecteur de contexte\n            context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_out).squeeze(1)\n            \n            # Stocker les poids d'attention\n            attention_weights_list.append(attention_weights.squeeze(0).cpu().numpy())\n            \n            # Un pas de décodage\n            lstm_input = torch.cat([word_embedded, context_vector], dim=1)\n            h, c = model.decoder.lstm_cell(lstm_input, (h, c))\n            \n            # Prédire le mot suivant\n            output = model.decoder.fc(model.decoder.dropout(h))\n            predicted_word = output.argmax(1)\n            word_idx = predicted_word.item()\n            \n            # Vérifier si c'est la fin\n            if word_idx == word2idx['<eos>']:\n                break\n            \n            # Ajouter le mot à la légende\n            word = idx2word[word_idx]\n            if word not in ['<pad>', '<unk>']:\n                caption_words.append(word)\n            \n            # Mettre à jour le mot d'entrée\n            input_word = predicted_word.unsqueeze(1)\n        \n        caption = ' '.join(caption_words)\n        attention_weights_array = np.array(attention_weights_list)  # (num_words, 49)\n        \n        return caption, attention_weights_array\n\n# Tester la visualisation de l'attention\nprint(\"\\nVisualisation de l'attention:\")\ntest_image, test_caption = test_dataset[0]\ncaption, attention_weights = visualize_attention(model, test_image, word2idx, idx2word)\n\nprint(f\"Légende générée: {caption}\")\nprint(f\"Shape des poids d'attention: {attention_weights.shape}\")\nprint(f\"Poids d'attention pour le premier mot: {attention_weights[0][:10]}...\")  # Afficher les 10 premiers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:48:18.535705Z","iopub.execute_input":"2025-12-25T11:48:18.536304Z","iopub.status.idle":"2025-12-25T11:48:18.574698Z","shell.execute_reply.started":"2025-12-25T11:48:18.536267Z","shell.execute_reply":"2025-12-25T11:48:18.574111Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Résumé final et conseils pour Kaggle**","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"RÉSUMÉ DU TP - IMAGE CAPTIONING AVEC ATTENTION\")\nprint(\"=\"*60)\nprint(f\"✔ Dataset: Flickr30k ({len(dataset)} images)\")\nprint(f\"✔ Vocabulaire: {vocab_size} mots\")\nprint(f\"✔ Modèle: ResNet50 + LSTM avec Attention\")\nprint(f\"✔ Embedding dimension: {embedding_dim}\")\nprint(f\"✔ Hidden dimension: {hidden_dim}\")\nprint(f\"✔ Batch size: {batch_size}\")\nprint(f\"✔ Époques d'entraînement: {num_epochs}\")\nprint(f\"✔ Device utilisé: {device}\")\nprint(\"=\"*60)\nprint(\"\\nConseils pour Kaggle:\")\nprint(\"1. Commencez avec peu d'époques (10-20) pour tester\")\nprint(\"2. Ajustez batch_size selon la mémoire disponible\")\nprint(\"3. Utilisez le gradient clipping pour stabiliser l'entraînement\")\nprint(\"4. Sauvegardez régulièrement votre modèle\")\nprint(\"5. Testez avec beam search pour de meilleurs résultats\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:48:24.317736Z","iopub.execute_input":"2025-12-25T11:48:24.318451Z","iopub.status.idle":"2025-12-25T11:48:24.324625Z","shell.execute_reply.started":"2025-12-25T11:48:24.318421Z","shell.execute_reply":"2025-12-25T11:48:24.323896Z"}},"outputs":[],"execution_count":null}]}